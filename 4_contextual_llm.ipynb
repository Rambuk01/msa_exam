{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ContextualSentimentAnalyzer:\n",
    "    def __init__(self, model=\"HuggingFaceH4/zephyr-7b-beta\", save_interval=10):\n",
    "        \"\"\"\n",
    "        Initialize the Mistral model and tokenizer for sentiment analysis.\n",
    "        \"\"\"\n",
    "        self.model_name = model\n",
    "        self.save_interval = save_interval\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "        print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "        # System context for the task\n",
    "        self.context = (\n",
    "            \"You are analyzing Reddit posts about GME stock to determine their sentiment.\\n\"\n",
    "            \"Classify each post strictly as one of the following:\\n\"\n",
    "            \"- Bullish: Positive sentiment, optimism, or support for GME.\\n\"\n",
    "            \"- Bearish: Negative sentiment, pessimism, or doubt about GME.\\n\"\n",
    "            \"- Neutral: No strong positive or negative sentiment.\\n\\n\"\n",
    "            \"Respond only with 'bullish', 'bearish', or 'neutral'.\"\n",
    "        )\n",
    "\n",
    "    def extract_sentiment(self, response):\n",
    "      \"\"\"\n",
    "      Extract sentiment label ('bullish', 'bearish', 'neutral') from the content after <|assistant|>.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          # Extract everything after <|assistant|>\n",
    "          assistant_response = re.split(r\"<\\|assistant\\|>\", response, maxsplit=1)\n",
    "          if len(assistant_response) < 2:\n",
    "              return \"uncertain\"  # Default if <|assistant|> is missing\n",
    "\n",
    "          # Clean and strip whitespace\n",
    "          clean_response = assistant_response[1].strip()\n",
    "          print(clean_response)\n",
    "          # Search for 'bull', 'bear', or 'neut' (to match variations like bullish, bearish, neutral)\n",
    "          match = re.search(r\"\\b(bullish\\w*|bearish\\w*|neutral\\w*)\\b\", clean_response.lower())\n",
    "          if match:\n",
    "              return match.group(1)  # Return the matching sentiment keyword\n",
    "          return \"uncertain\"  # If no sentiment is found\n",
    "      except Exception as e:\n",
    "          print(f\"Error extracting sentiment: {e}\")\n",
    "          return \"error\"\n",
    "\n",
    "    def analyze_post(self, post):\n",
    "        \"\"\"\n",
    "        Analyze a post with contextual prompting and return a sentiment label.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format the messages in chat format\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.context},\n",
    "                {\"role\": \"user\", \"content\": f\"Reddit Post: '{post}'\\nSentiment:\"}\n",
    "            ]\n",
    "            # Tokenize the messages using the chat template\n",
    "            model_inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            # Generate the model response\n",
    "            generated_ids = self.model.generate(model_inputs, max_new_tokens=20, do_sample=False, temperature=0.2)\n",
    "            response = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            print(f\"DEBUG: Raw Model Response: '{response}'\")\n",
    "            print(f\"DEBUG_END\")\n",
    "            # Extract sentiment label using regex\n",
    "            sentiment = self.extract_sentiment(response)\n",
    "            return sentiment\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post: {e}\")\n",
    "            return \"error\"\n",
    "\n",
    "\n",
    "    def process_dataframe(self, df, text_column, output_file):\n",
    "\n",
    "        \"\"\"\n",
    "        Process the DataFrame row by row and save progress periodically.\n",
    "        \"\"\"\n",
    "        if 'sentiment' not in df.columns:\n",
    "            df['sentiment'] = None\n",
    "\n",
    "        processed_count = 0\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            torch.cuda.empty_cache()  # Free up GPU memory (optional)\n",
    "\n",
    "            if pd.notna(row['sentiment']):\n",
    "                continue  # Skip already processed rows\n",
    "\n",
    "            print(f\"\\nProcessing row {i + 1}/{len(df)}...\")\n",
    "            sentiment = self.analyze_post(row[text_column])\n",
    "            df.at[i, 'sentiment'] = sentiment\n",
    "            print(f\"Post: {row[text_column]}\\nSentiment: {sentiment}\")\n",
    "\n",
    "            processed_count += 1\n",
    "\n",
    "            # Save progress every SAVE_INTERVAL rows\n",
    "            if processed_count % self.save_interval == 0:\n",
    "                df.to_csv(output_file, index=False)\n",
    "                print(f\"Progress saved after {processed_count} rows.\")\n",
    "\n",
    "        # Final save\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(\"Sentiment analysis complete! Final file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('data/sentiment_ready_2.csv')\n",
    "output_file = \"data/sentiment_done_2.csv\"\n",
    "\n",
    "\n",
    "# Initialize the ContextualSentimentAnalyzer\n",
    "sentiment_analyzer = ContextualSentimentAnalyzer(\n",
    "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    save_interval=10\n",
    ")\n",
    "# Process the DataFrame\n",
    "sentiment_analyzer.process_dataframe(df=df, text_column='post', output_file=output_file)\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
