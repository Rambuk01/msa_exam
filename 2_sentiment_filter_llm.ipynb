{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1734227190942,
     "user": {
      "displayName": "Mario F.",
      "userId": "05098247898487122569"
     },
     "user_tz": -60
    },
    "id": "CpOBU5eJVc8i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from google.colab import drive\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26546,
     "status": "ok",
     "timestamp": 1734227221894,
     "user": {
      "displayName": "Mario F.",
      "userId": "05098247898487122569"
     },
     "user_tz": -60
    },
    "id": "TcF2eWLvUguH",
    "outputId": "f992e8c7-a9e6-4342-8598-5bf6824a0700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount the google drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd '/content/gdrive/MyDrive/SDU/DS821 - Market Sentiment Analysis/News_and_Market_Sentiment_Analytics-main/exam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1734227268006,
     "user": {
      "displayName": "Mario F.",
      "userId": "05098247898487122569"
     },
     "user_tz": -60
    },
    "id": "6f344tbHnasy"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class StockPostFilterLLM:\n",
    "    def __init__(self, model=\"HuggingFaceH4/zephyr-7b-beta\"):\n",
    "        self.model_name = model\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        self.pipe = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "        # Context for the model\n",
    "        self.context = \"\"\"\n",
    "          You are a financial expert. Your task is to determine if a Reddit post could influence GME stock.\n",
    "          Look for mentions of GME, price predictions, news, or strong bullish/bearish sentiment.\n",
    "          Answer 'yes' if the post is likely to impact GME's price or trading activity, otherwise answer 'no'.\"\"\"\n",
    "\n",
    "    def construct_message(self, context, post):\n",
    "        \"\"\"\n",
    "        Construct a prompt from the given context and prompt.\n",
    "        \"\"\"\n",
    "        # We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "        message = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": context,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": post\n",
    "            },\n",
    "        ]\n",
    "        return message\n",
    "    def check_response_for_yes(self, response):\n",
    "      # Regex pattern to ensure 'yes' appears after <|assistant|>\n",
    "      pattern = r'<\\|assistant\\|>.*?\\byes\\b'\n",
    "      return 1 if re.search(pattern, response, re.IGNORECASE | re.DOTALL) else 0\n",
    "    \n",
    "    def __call__(self, post, temperature=0.7):\n",
    "\n",
    "      # Format the messages in chat format\n",
    "      message = self.construct_message(self.context, post)\n",
    "      prompt = self.pipe.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "      outputs = self.pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "      resp = outputs[0][\"generated_text\"]\n",
    "      print(resp)\n",
    "      print(f\"----- {self.check_response_for_yes(resp)} -----\")\n",
    "      # Extract sentiment label using regex\n",
    "      return self.check_response_for_yes(resp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7RuMlSTwB0M"
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm_filter = StockPostFilterLLM()\n",
    "\n",
    "df = pd.read_csv('data/rd_clean.csv', sep=',')\n",
    "df = df[df['gme'] == 1]\n",
    "df[\"sentiment\"] = np.nan  # New column to store results\n",
    "\n",
    "# Set a batch save interval to improve performance\n",
    "SAVE_INTERVAL = 10  # Save progress every 10 rows\n",
    "processed_count = 0  # Track processed rows for saving\n",
    "\n",
    "# Loop through the rows\n",
    "for i, row in df.iterrows():\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache to free memory\n",
    "\n",
    "    # Skip already processed rows\n",
    "    if not pd.isna(row['sentiment']):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing row {i+1}/{len(df)}...\")\n",
    "\n",
    "    # Generate sentiment\n",
    "    sentiment_result = llm_filter(row['body'])\n",
    "    df.at[i, 'sentiment'] = sentiment_result  # Update DataFrame\n",
    "\n",
    "    # Increment processed count\n",
    "    processed_count += 1\n",
    "\n",
    "    # Save progress in batches\n",
    "    if processed_count % SAVE_INTERVAL == 0:\n",
    "        df.to_csv('data/df_gme_sentiment_fixed.csv', index=False)\n",
    "        print(f\"Progress saved after {processed_count} rows.\")\n",
    "\n",
    "# Final save\n",
    "df.to_csv('data/df_gme_sentiment_fixed.csv', index=False)\n",
    "print(\"Sentiment analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMNHveCxSL2sSPgBPWGPMhy",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
