{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1734227190942,
     "user": {
      "displayName": "Mario F.",
      "userId": "05098247898487122569"
     },
     "user_tz": -60
    },
    "id": "CpOBU5eJVc8i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from google.colab import drive\n",
    "\n",
    "# Login to hugging face. Required for Mistral.\n",
    "token = 'MY_TOKEN'\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26546,
     "status": "ok",
     "timestamp": 1734227221894,
     "user": {
      "displayName": "Mario F.",
      "userId": "05098247898487122569"
     },
     "user_tz": -60
    },
    "id": "TcF2eWLvUguH",
    "outputId": "f992e8c7-a9e6-4342-8598-5bf6824a0700"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "# Mount the google drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd '/content/gdrive/MyDrive/SDU/DS821 - Market Sentiment Analysis/News_and_Market_Sentiment_Analytics-main/exam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1734227268006,
     "user": {
      "displayName": "Mario F.",
      "userId": "05098247898487122569"
     },
     "user_tz": -60
    },
    "id": "6f344tbHnasy"
   },
   "outputs": [],
   "source": [
    "class StockPostFilterLLM:\n",
    "    def __init__(self, model=\"mistralai/Mistral-7B-v0.1\"):\n",
    "        self.model = model\n",
    "        \n",
    "        # Context for the model\n",
    "        self.context = \"\"\"\n",
    "          You are a financial expert. Your task is to determine if a Reddit post could influence GME stock.\n",
    "          Look for mentions of GME, price predictions, news, or strong bullish/bearish sentiment.\n",
    "          Answer 'yes' if the post is likely to impact GME's price or trading activity, otherwise answer 'no'.\"\"\"\n",
    "\n",
    "        print(\"Starting model loading...\")\n",
    "        self.pipe = self.load_model(self.model)\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def load_model(self, model):\n",
    "        \"\"\"\n",
    "        Load the pretrained language model using Hugging Face pipeline.\n",
    "        \"\"\"\n",
    "        print(f\"Loading model '{model}' with device_map='auto'...\")\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"Pipeline initialized.\")\n",
    "        return pipe\n",
    "\n",
    "    def construct_prompt(self, post):\n",
    "        \"\"\"\n",
    "        Construct a prompt by combining the system context and the Reddit post.\n",
    "        \"\"\"\n",
    "        print(\"Constructing the prompt...\")\n",
    "        prompt = f\"{self.context}\\nReddit post: '{post}'\\nResponse:\"\n",
    "        print(f\"Prompt constructed.\")\n",
    "        return prompt\n",
    "\n",
    "    def __call__(self, post, temperature=0.5):\n",
    "        \"\"\"\n",
    "        Generate a response to determine if the post might influence the stock market.\n",
    "        \"\"\"\n",
    "        print(\"Calling LLM with the post...\")\n",
    "        prompt = self.construct_prompt(post)\n",
    "        print(\"Starting text generation...\")\n",
    "        outputs = self.pipe(prompt, max_new_tokens=15, do_sample=True, temperature=temperature, top_p=0.95)\n",
    "        print(\"Text generation complete!\")\n",
    "        return outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7RuMlSTwB0M"
   },
   "outputs": [],
   "source": [
    "# Initialize the LLM model\n",
    "llm_filter = StockPostFilterLLM(model=\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "df = pd.read_csv('data/rd_clean.csv', sep=',')\n",
    "df = df[df['gme'] == 1]\n",
    "df[\"sentiment\"] = np.nan  # New column to store results\n",
    "\n",
    "# Set a save interval to improve performance\n",
    "SAVE_INTERVAL = 10  # Save progress every 10 rows\n",
    "processed_count = 0  # Track processed rows for saving\n",
    "\n",
    "# Loop through the rows\n",
    "for i, row in df.iterrows():\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache to free memory\n",
    "\n",
    "    # Skip already processed rows\n",
    "    if not pd.isna(row['sentiment']):\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nProcessing row {i+1}/{len(df)}...\")\n",
    "\n",
    "    try:\n",
    "        # Generate sentiment\n",
    "        sentiment_result = llm_filter(row['body'])\n",
    "        df.at[i, 'sentiment'] = sentiment_result  # Update DataFrame\n",
    "        print(f\"Post: {row['body']}\\nSentiment: {sentiment_result}\")\n",
    "\n",
    "        # Increment processed count\n",
    "        processed_count += 1\n",
    "\n",
    "        # Save progress in batches\n",
    "        if processed_count % SAVE_INTERVAL == 0:\n",
    "            df.to_csv('data/df_gme_sentiment.csv', index=False)\n",
    "            print(f\"Progress saved after {processed_count} rows.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        df.at[i, 'sentiment'] = \"Error\"  # Log error in the sentiment column\n",
    "        torch.cuda.empty_cache()  # Clear cache after failure to ensure stability\n",
    "\n",
    "# Final save\n",
    "df.to_csv('data/df_gme_sentiment_fixed.csv', index=False)\n",
    "print(\"Sentiment analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMNHveCxSL2sSPgBPWGPMhy",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
